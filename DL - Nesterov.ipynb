{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ddcfc6e3",
   "metadata": {},
   "source": [
    "# Entrenamiento de Modelo con Nesterov Accelerated Gradient (NAG)\n",
    "\n",
    "Este notebook implementa un modelo de regresión usando **Nesterov Accelerated Gradient (NAG)** para predecir valores basados en datos de series temporales del archivo `dryer.dat`.\n",
    "\n",
    "## ¿Qué es Nesterov Accelerated Gradient?\n",
    "\n",
    "NAG es una mejora sobre el Momentum estándar que **anticipa** la dirección del gradiente antes de calcularlo. Es como \"mirar hacia adelante\" antes de dar el paso.\n",
    "\n",
    "### Diferencia clave con SGD estándar:\n",
    "- **SGD:** Calcula el gradiente en la posición actual\n",
    "- **Momentum:** Acumula velocidad de gradientes pasados\n",
    "- **Nesterov:** Calcula el gradiente en la posición anticipada (más inteligente)\n",
    "\n",
    "## División de datos:\n",
    "- **70%** - Entrenamiento (Train)\n",
    "- **20%** - Validación (Valid)\n",
    "- **10%** - Prueba (Test)\n",
    "\n",
    "**Importante**: Los datos se dividen de forma secuencial para respetar el orden temporal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b5a3f6",
   "metadata": {},
   "source": [
    "## 1. Importar Librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6042eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eeeed83",
   "metadata": {},
   "source": [
    "## 2. Cargar y Explorar los Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "191b5362",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando datos desde dryer.dat...\n",
      "Datos cargados: 1000 muestras\n",
      "Forma de X: (1000, 1)\n",
      "Forma de y: (1000, 1)\n"
     ]
    }
   ],
   "source": [
    "# Cargar datos desde dryer.dat\n",
    "print(\"Cargando datos desde dryer.dat...\")\n",
    "data = np.loadtxt('dryer.dat')\n",
    "\n",
    "# Separar características (X) y variable objetivo (y)\n",
    "X = data[:, 0].reshape(-1, 1)  # Primera columna como entrada\n",
    "y = data[:, 1].reshape(-1, 1)  # Segunda columna como salida\n",
    "\n",
    "print(f\"Datos cargados: {len(X)} muestras\")\n",
    "print(f\"Forma de X: {X.shape}\")\n",
    "print(f\"Forma de y: {y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd884448",
   "metadata": {},
   "source": [
    "## 3. División Secuencial de los Datos\n",
    "\n",
    "Como son datos de series temporales, NO mezclamos los datos. Mantenemos el orden temporal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3af60df8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "División de datos SECUENCIAL (series temporales):\n",
      "Train: 700 muestras (70.0%) - Índices [0:700]\n",
      "Valid: 200 muestras (20.0%) - Índices [700:900]\n",
      "Test:  100 muestras (10.0%) - Índices [900:1000]\n"
     ]
    }
   ],
   "source": [
    "# División de datos SECUENCIAL (para series temporales): 70% train, 20% valid, 10% test\n",
    "# No mezclamos los datos, mantenemos el orden temporal\n",
    "n_samples = len(X)\n",
    "train_end = int(0.7 * n_samples)\n",
    "valid_end = int(0.9 * n_samples)\n",
    "\n",
    "# División secuencial\n",
    "X_train = X[:train_end]\n",
    "y_train = y[:train_end]\n",
    "\n",
    "X_valid = X[train_end:valid_end]\n",
    "y_valid = y[train_end:valid_end]\n",
    "\n",
    "X_test = X[valid_end:]\n",
    "y_test = y[valid_end:]\n",
    "\n",
    "print(f\"\\nDivisión de datos SECUENCIAL (series temporales):\")\n",
    "print(f\"Train: {len(X_train)} muestras ({len(X_train)/len(X)*100:.1f}%) - Índices [0:{train_end}]\")\n",
    "print(f\"Valid: {len(X_valid)} muestras ({len(X_valid)/len(X)*100:.1f}%) - Índices [{train_end}:{valid_end}]\")\n",
    "print(f\"Test:  {len(X_test)} muestras ({len(X_test)/len(X)*100:.1f}%) - Índices [{valid_end}:{n_samples}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11151012",
   "metadata": {},
   "source": [
    "## 4. Normalización/Estandarización de los Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1918c2f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datos normalizados exitosamente\n"
     ]
    }
   ],
   "source": [
    "# Normalizar datos usando StandardScaler\n",
    "scaler_X = StandardScaler()\n",
    "scaler_y = StandardScaler()\n",
    "\n",
    "X_train_norm = scaler_X.fit_transform(X_train)\n",
    "X_valid_norm = scaler_X.transform(X_valid)\n",
    "X_test_norm = scaler_X.transform(X_test)\n",
    "\n",
    "y_train_norm = scaler_y.fit_transform(y_train)\n",
    "y_valid_norm = scaler_y.transform(y_valid)\n",
    "y_test_norm = scaler_y.transform(y_test)\n",
    "\n",
    "print(\"Datos normalizados exitosamente\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "711fed33",
   "metadata": {},
   "source": [
    "## 5. Implementación del Modelo con Nesterov Accelerated Gradient\n",
    "\n",
    "Implementamos desde cero un modelo de regresión usando **Nesterov Accelerated Gradient (NAG)**.\n",
    "\n",
    "### Fórmula de Nesterov:\n",
    "```\n",
    "v_t = momentum * v_{t-1} + learning_rate * gradiente(θ - momentum * v_{t-1})\n",
    "θ_t = θ_{t-1} - v_t\n",
    "```\n",
    "\n",
    "Donde:\n",
    "- `v_t` es la velocidad (acumulación de gradientes)\n",
    "- `momentum` es típicamente 0.9\n",
    "- El gradiente se calcula en una posición **anticipada**: `θ - momentum * v_{t-1}`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "898935e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clase NesterovRegressor definida exitosamente\n"
     ]
    }
   ],
   "source": [
    "# Implementación de Nesterov Accelerated Gradient (NAG)\n",
    "class NesterovRegressor:\n",
    "    def __init__(self, learning_rate=0.01, n_epochs=700, batch_size=32, momentum=0.9):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_epochs = n_epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.momentum = momentum  # Factor de momentum (típicamente 0.9)\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "        self.velocity_w = None  # Velocidad para los pesos\n",
    "        self.velocity_b = None  # Velocidad para el bias\n",
    "        self.train_losses = []\n",
    "        self.valid_losses = []\n",
    "        \n",
    "    def _initialize_parameters(self, n_features):\n",
    "        \"\"\"Inicializar pesos, bias y velocidades\"\"\"\n",
    "        self.weights = np.random.randn(n_features, 1) * 0.01\n",
    "        self.bias = np.zeros((1, 1))\n",
    "        # Inicializar velocidades en cero\n",
    "        self.velocity_w = np.zeros((n_features, 1))\n",
    "        self.velocity_b = np.zeros((1, 1))\n",
    "    \n",
    "    def _compute_loss(self, y_true, y_pred):\n",
    "        \"\"\"Calcular MSE (Mean Squared Error)\"\"\"\n",
    "        return np.mean((y_true - y_pred) ** 2)\n",
    "    \n",
    "    def _forward(self, X, weights=None, bias=None):\n",
    "        \"\"\"Propagación hacia adelante\n",
    "        \n",
    "        Args:\n",
    "            X: datos de entrada\n",
    "            weights: pesos a usar (si es None, usa self.weights)\n",
    "            bias: bias a usar (si es None, usa self.bias)\n",
    "        \"\"\"\n",
    "        if weights is None:\n",
    "            weights = self.weights\n",
    "        if bias is None:\n",
    "            bias = self.bias\n",
    "        return np.dot(X, weights) + bias\n",
    "    \n",
    "    def _backward(self, X, y_true, y_pred):\n",
    "        \"\"\"Propagación hacia atrás (calcular gradientes)\"\"\"\n",
    "        n_samples = X.shape[0]\n",
    "        \n",
    "        # Gradientes\n",
    "        dw = (-2/n_samples) * np.dot(X.T, (y_true - y_pred))\n",
    "        db = (-2/n_samples) * np.sum(y_true - y_pred)\n",
    "        \n",
    "        return dw, db\n",
    "    \n",
    "    def fit(self, X_train, y_train, X_valid=None, y_valid=None):\n",
    "        \"\"\"Entrenar el modelo usando Nesterov Accelerated Gradient\"\"\"\n",
    "        n_samples, n_features = X_train.shape\n",
    "        self._initialize_parameters(n_features)\n",
    "        \n",
    "        # Entrenar por épocas\n",
    "        for epoch in range(self.n_epochs):\n",
    "            X = X_train\n",
    "            Y = y_train\n",
    "            \n",
    "            # Mini-batch NAG\n",
    "            for i in range(0, n_samples, self.batch_size):\n",
    "                X_batch = X[i:i+self.batch_size]\n",
    "                y_batch = Y[i:i+self.batch_size]\n",
    "                \n",
    "                # ===== PASO 1: Calcular posición anticipada (lookahead) =====\n",
    "                # Esta es la clave de Nesterov: calcular gradiente en la posición futura\n",
    "                weights_lookahead = self.weights - self.momentum * self.velocity_w\n",
    "                bias_lookahead = self.bias - self.momentum * self.velocity_b\n",
    "                \n",
    "                # ===== PASO 2: Forward pass en posición anticipada =====\n",
    "                y_pred = self._forward(X_batch, weights_lookahead, bias_lookahead)\n",
    "                \n",
    "                # ===== PASO 3: Backward pass (gradientes en posición anticipada) =====\n",
    "                dw, db = self._backward(X_batch, y_batch, y_pred)\n",
    "                \n",
    "                # ===== PASO 4: Actualizar velocidades =====\n",
    "                self.velocity_w = self.momentum * self.velocity_w + self.learning_rate * dw\n",
    "                self.velocity_b = self.momentum * self.velocity_b + self.learning_rate * db\n",
    "                \n",
    "                # ===== PASO 5: Actualizar parámetros usando las velocidades =====\n",
    "                self.weights -= self.velocity_w\n",
    "                self.bias -= self.velocity_b\n",
    "            \n",
    "            # Calcular pérdida en train\n",
    "            y_train_pred = self._forward(X_train)\n",
    "            train_loss = self._compute_loss(y_train, y_train_pred)\n",
    "            self.train_losses.append(train_loss)\n",
    "            \n",
    "            # Calcular pérdida en validación (si se proporciona)\n",
    "            if X_valid is not None and y_valid is not None:\n",
    "                y_valid_pred = self._forward(X_valid)\n",
    "                valid_loss = self._compute_loss(y_valid, y_valid_pred)\n",
    "                self.valid_losses.append(valid_loss)\n",
    "                \n",
    "                if (epoch + 1) % 10 == 0:\n",
    "                    print(f\"Época {epoch+1}/{self.n_epochs} - Train Loss: {train_loss:.6f} - Valid Loss: {valid_loss:.6f}\")\n",
    "            else:\n",
    "                if (epoch + 1) % 10 == 0:\n",
    "                    print(f\"Época {epoch+1}/{self.n_epochs} - Train Loss: {train_loss:.6f}\")\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Hacer predicciones\"\"\"\n",
    "        return self._forward(X)\n",
    "    \n",
    "    def evaluate(self, X, y):\n",
    "        \"\"\"Evaluar el modelo\"\"\"\n",
    "        y_pred = self.predict(X)\n",
    "        mse = self._compute_loss(y, y_pred)\n",
    "        rmse = np.sqrt(mse)\n",
    "        \n",
    "        # R² score\n",
    "        ss_tot = np.sum((y - np.mean(y)) ** 2)\n",
    "        ss_res = np.sum((y - y_pred) ** 2)\n",
    "        r2 = 1 - (ss_res / ss_tot)\n",
    "        \n",
    "        return {'MSE': mse, 'RMSE': rmse, 'R2': r2}\n",
    "\n",
    "print(\"Clase NesterovRegressor definida exitosamente\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7467790b",
   "metadata": {},
   "source": [
    "## 6. Entrenar el Modelo con Nesterov\n",
    "\n",
    "### Hiperparámetros:\n",
    "- **learning_rate**: 0.01 (tasa de aprendizaje)\n",
    "- **n_epochs**: 200 (número de épocas)\n",
    "- **batch_size**: 32 (tamaño del mini-batch)\n",
    "- **momentum**: 0.9 (factor de aceleración de Nesterov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c7bba659",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ENTRENAMIENTO DEL MODELO NESTEROV ACCELERATED GRADIENT\n",
      "============================================================\n",
      "Época 10/200 - Train Loss: 0.971394 - Valid Loss: 0.867320\n",
      "Época 20/200 - Train Loss: 0.971394 - Valid Loss: 0.867320\n",
      "Época 30/200 - Train Loss: 0.971394 - Valid Loss: 0.867320\n",
      "Época 40/200 - Train Loss: 0.971394 - Valid Loss: 0.867320\n",
      "Época 50/200 - Train Loss: 0.971394 - Valid Loss: 0.867320\n",
      "Época 60/200 - Train Loss: 0.971394 - Valid Loss: 0.867320\n",
      "Época 70/200 - Train Loss: 0.971394 - Valid Loss: 0.867320\n",
      "Época 80/200 - Train Loss: 0.971394 - Valid Loss: 0.867320\n",
      "Época 90/200 - Train Loss: 0.971394 - Valid Loss: 0.867320\n",
      "Época 100/200 - Train Loss: 0.971394 - Valid Loss: 0.867320\n",
      "Época 110/200 - Train Loss: 0.971394 - Valid Loss: 0.867320\n",
      "Época 120/200 - Train Loss: 0.971394 - Valid Loss: 0.867320\n",
      "Época 130/200 - Train Loss: 0.971394 - Valid Loss: 0.867320\n",
      "Época 140/200 - Train Loss: 0.971394 - Valid Loss: 0.867320\n",
      "Época 150/200 - Train Loss: 0.971394 - Valid Loss: 0.867320\n",
      "Época 160/200 - Train Loss: 0.971394 - Valid Loss: 0.867320\n",
      "Época 170/200 - Train Loss: 0.971394 - Valid Loss: 0.867320\n",
      "Época 180/200 - Train Loss: 0.971394 - Valid Loss: 0.867320\n",
      "Época 190/200 - Train Loss: 0.971394 - Valid Loss: 0.867320\n",
      "Época 200/200 - Train Loss: 0.971394 - Valid Loss: 0.867320\n"
     ]
    }
   ],
   "source": [
    "# Crear y entrenar el modelo con Nesterov\n",
    "print(\"=\"*60)\n",
    "print(\"ENTRENAMIENTO DEL MODELO NESTEROV ACCELERATED GRADIENT\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "model = NesterovRegressor(learning_rate=0.01, n_epochs=200, batch_size=32, momentum=0.9)\n",
    "model.fit(X_train_norm, y_train_norm, X_valid_norm, y_valid_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1c7e42",
   "metadata": {},
   "source": [
    "## 7. Evaluación del Modelo\n",
    "\n",
    "### 7.1 Evaluación en Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9d9ad931",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "EVALUACIÓN EN CONJUNTO DE ENTRENAMIENTO\n",
      "============================================================\n",
      "MSE: 0.971394\n",
      "RMSE: 0.985593\n",
      "R2: 0.028606\n"
     ]
    }
   ],
   "source": [
    "# Evaluar en conjunto de entrenamiento\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EVALUACIÓN EN CONJUNTO DE ENTRENAMIENTO\")\n",
    "print(\"=\"*60)\n",
    "train_metrics = model.evaluate(X_train_norm, y_train_norm)\n",
    "for metric, value in train_metrics.items():\n",
    "    print(f\"{metric}: {value:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd908b2",
   "metadata": {},
   "source": [
    "### 7.2 Evaluación en Validación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0e48e221",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "EVALUACIÓN EN CONJUNTO DE VALIDACIÓN\n",
      "============================================================\n",
      "MSE: 0.867320\n",
      "RMSE: 0.931300\n",
      "R2: -0.070905\n"
     ]
    }
   ],
   "source": [
    "# Evaluar en conjunto de validación\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EVALUACIÓN EN CONJUNTO DE VALIDACIÓN\")\n",
    "print(\"=\"*60)\n",
    "valid_metrics = model.evaluate(X_valid_norm, y_valid_norm)\n",
    "for metric, value in valid_metrics.items():\n",
    "    print(f\"{metric}: {value:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e989adde",
   "metadata": {},
   "source": [
    "### 7.3 Evaluación en Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0fea221d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "EVALUACIÓN EN CONJUNTO DE PRUEBA (TEST)\n",
      "============================================================\n",
      "MSE: 0.751478\n",
      "RMSE: 0.866878\n",
      "R2: -0.059379\n"
     ]
    }
   ],
   "source": [
    "# Evaluar en conjunto de prueba\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EVALUACIÓN EN CONJUNTO DE PRUEBA (TEST)\")\n",
    "print(\"=\"*60)\n",
    "test_metrics = model.evaluate(X_test_norm, y_test_norm)\n",
    "for metric, value in test_metrics.items():\n",
    "    print(f\"{metric}: {value:.6f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
