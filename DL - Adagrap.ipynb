{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ddcfc6e3",
   "metadata": {},
   "source": [
    "# Entrenamiento de Modelo con AdaGrad (Adaptive Gradient)\n",
    "\n",
    "Este notebook implementa un modelo de regresión usando **AdaGrad (Adaptive Gradient Algorithm)** para predecir valores basados en datos de series temporales del archivo `dryer.dat`.\n",
    "\n",
    "## ¿Qué es AdaGrad?\n",
    "\n",
    "AdaGrad es un algoritmo de optimización adaptativo que **ajusta el learning rate de forma individual para cada parámetro**. Los parámetros que reciben gradientes grandes tienen su learning rate reducido, mientras que los parámetros con gradientes pequeños mantienen un learning rate mayor.\n",
    "\n",
    "### Características clave:\n",
    "- **Learning rates adaptativos**: Cada peso tiene su propio learning rate\n",
    "- **Ideal para datos esparsos**: Funciona muy bien en NLP y datos categóricos\n",
    "- **Sin necesidad de ajuste manual**: El learning rate se adapta automáticamente\n",
    "- **Acumulación de gradientes**: Mantiene un historial de gradientes al cuadrado\n",
    "\n",
    "## División de datos:\n",
    "- **70%** - Entrenamiento (Train)\n",
    "- **20%** - Validación (Valid)\n",
    "- **10%** - Prueba (Test)\n",
    "\n",
    "**Importante**: Los datos se dividen de forma secuencial para respetar el orden temporal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b5a3f6",
   "metadata": {},
   "source": [
    "## 1. Importar Librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6042eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eeeed83",
   "metadata": {},
   "source": [
    "## 2. Cargar y Explorar los Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "191b5362",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando datos desde dryer.dat...\n",
      "Datos cargados: 1000 muestras\n",
      "Forma de X: (1000, 1)\n",
      "Forma de y: (1000, 1)\n"
     ]
    }
   ],
   "source": [
    "# Cargar datos desde dryer.dat\n",
    "print(\"Cargando datos desde dryer.dat...\")\n",
    "data = np.loadtxt('dryer.dat')\n",
    "\n",
    "# Separar características (X) y variable objetivo (y)\n",
    "X = data[:, 0].reshape(-1, 1)  # Primera columna como entrada\n",
    "y = data[:, 1].reshape(-1, 1)  # Segunda columna como salida\n",
    "\n",
    "print(f\"Datos cargados: {len(X)} muestras\")\n",
    "print(f\"Forma de X: {X.shape}\")\n",
    "print(f\"Forma de y: {y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd884448",
   "metadata": {},
   "source": [
    "## 3. División Secuencial de los Datos\n",
    "\n",
    "Como son datos de series temporales, NO mezclamos los datos. Mantenemos el orden temporal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3af60df8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "División de datos SECUENCIAL (series temporales):\n",
      "Train: 700 muestras (70.0%) - Índices [0:700]\n",
      "Valid: 200 muestras (20.0%) - Índices [700:900]\n",
      "Test:  100 muestras (10.0%) - Índices [900:1000]\n"
     ]
    }
   ],
   "source": [
    "# División de datos SECUENCIAL (para series temporales): 70% train, 20% valid, 10% test\n",
    "# No mezclamos los datos, mantenemos el orden temporal\n",
    "n_samples = len(X)\n",
    "train_end = int(0.7 * n_samples)\n",
    "valid_end = int(0.9 * n_samples)\n",
    "\n",
    "# División secuencial\n",
    "X_train = X[:train_end]\n",
    "y_train = y[:train_end]\n",
    "\n",
    "X_valid = X[train_end:valid_end]\n",
    "y_valid = y[train_end:valid_end]\n",
    "\n",
    "X_test = X[valid_end:]\n",
    "y_test = y[valid_end:]\n",
    "\n",
    "print(f\"\\nDivisión de datos SECUENCIAL (series temporales):\")\n",
    "print(f\"Train: {len(X_train)} muestras ({len(X_train)/len(X)*100:.1f}%) - Índices [0:{train_end}]\")\n",
    "print(f\"Valid: {len(X_valid)} muestras ({len(X_valid)/len(X)*100:.1f}%) - Índices [{train_end}:{valid_end}]\")\n",
    "print(f\"Test:  {len(X_test)} muestras ({len(X_test)/len(X)*100:.1f}%) - Índices [{valid_end}:{n_samples}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11151012",
   "metadata": {},
   "source": [
    "## 4. Normalización/Estandarización de los Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1918c2f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datos normalizados exitosamente\n"
     ]
    }
   ],
   "source": [
    "# Normalizar datos usando StandardScaler\n",
    "scaler_X = StandardScaler()\n",
    "scaler_y = StandardScaler()\n",
    "\n",
    "X_train_norm = scaler_X.fit_transform(X_train)\n",
    "X_valid_norm = scaler_X.transform(X_valid)\n",
    "X_test_norm = scaler_X.transform(X_test)\n",
    "\n",
    "y_train_norm = scaler_y.fit_transform(y_train)\n",
    "y_valid_norm = scaler_y.transform(y_valid)\n",
    "y_test_norm = scaler_y.transform(y_test)\n",
    "\n",
    "print(\"Datos normalizados exitosamente\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "711fed33",
   "metadata": {},
   "source": [
    "## 5. Implementación del Modelo con AdaGrad\n",
    "\n",
    "Implementamos desde cero un modelo de regresión usando **AdaGrad (Adaptive Gradient Algorithm)**.\n",
    "\n",
    "### Fórmula de AdaGrad:\n",
    "```\n",
    "G_t = G_{t-1} + (∇L)²           # Acumular gradientes al cuadrado\n",
    "θ_t = θ_{t-1} - (α / √(G_t + ε)) * ∇L\n",
    "```\n",
    "\n",
    "Donde:\n",
    "- `G_t` es la suma acumulada de gradientes al cuadrado\n",
    "- `α` es el learning rate inicial\n",
    "- `ε` es un término pequeño para evitar división por cero (típicamente 1e-8)\n",
    "- Cada parámetro tiene su propio `G_t`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "898935e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clase AdaGradRegressor definida exitosamente\n"
     ]
    }
   ],
   "source": [
    "# Implementación de AdaGrad (Adaptive Gradient Algorithm)\n",
    "class AdaGradRegressor:\n",
    "    def __init__(self, learning_rate=0.1, n_epochs=700, batch_size=32, epsilon=1e-8):\n",
    "        self.learning_rate = learning_rate  # Learning rate inicial (típicamente más alto que SGD)\n",
    "        self.n_epochs = n_epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.epsilon = epsilon  # Término pequeño para estabilidad numérica\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "        self.G_w = None  # Acumulador de gradientes al cuadrado para pesos\n",
    "        self.G_b = None  # Acumulador de gradientes al cuadrado para bias\n",
    "        self.train_losses = []\n",
    "        self.valid_losses = []\n",
    "        \n",
    "    def _initialize_parameters(self, n_features):\n",
    "        \"\"\"Inicializar pesos, bias y acumuladores\"\"\"\n",
    "        self.weights = np.random.randn(n_features, 1) * 0.01\n",
    "        self.bias = np.zeros((1, 1))\n",
    "        # CLAVE DE ADAGRAD: Inicializar acumuladores en cero\n",
    "        self.G_w = np.zeros((n_features, 1))\n",
    "        self.G_b = np.zeros((1, 1))\n",
    "    \n",
    "    def _compute_loss(self, y_true, y_pred):\n",
    "        \"\"\"Calcular MSE (Mean Squared Error)\"\"\"\n",
    "        return np.mean((y_true - y_pred) ** 2)\n",
    "    \n",
    "    def _forward(self, X):\n",
    "        \"\"\"Propagación hacia adelante\"\"\"\n",
    "        return np.dot(X, self.weights) + self.bias\n",
    "    \n",
    "    def _backward(self, X, y_true, y_pred):\n",
    "        \"\"\"Propagación hacia atrás (calcular gradientes)\"\"\"\n",
    "        n_samples = X.shape[0]\n",
    "        \n",
    "        # Gradientes\n",
    "        dw = (-2/n_samples) * np.dot(X.T, (y_true - y_pred))\n",
    "        db = (-2/n_samples) * np.sum(y_true - y_pred)\n",
    "        \n",
    "        return dw, db\n",
    "    \n",
    "    def fit(self, X_train, y_train, X_valid=None, y_valid=None):\n",
    "        \"\"\"Entrenar el modelo usando AdaGrad\"\"\"\n",
    "        n_samples, n_features = X_train.shape\n",
    "        self._initialize_parameters(n_features)\n",
    "        \n",
    "        # Entrenar por épocas\n",
    "        for epoch in range(self.n_epochs):\n",
    "            X = X_train\n",
    "            Y = y_train\n",
    "            \n",
    "            # Mini-batch AdaGrad\n",
    "            for i in range(0, n_samples, self.batch_size):\n",
    "                X_batch = X[i:i+self.batch_size]\n",
    "                y_batch = Y[i:i+self.batch_size]\n",
    "                \n",
    "                # ===== PASO 1: Forward pass =====\n",
    "                y_pred = self._forward(X_batch)\n",
    "                \n",
    "                # ===== PASO 2: Backward pass (calcular gradientes) =====\n",
    "                dw, db = self._backward(X_batch, y_batch, y_pred)\n",
    "                \n",
    "                # ===== PASO 3: Acumular gradientes al cuadrado (CLAVE DE ADAGRAD) =====\n",
    "                # G_t = G_{t-1} + (∇L)²\n",
    "                self.G_w += dw ** 2  # Elemento por elemento\n",
    "                self.G_b += db ** 2\n",
    "                \n",
    "                # ===== PASO 4: Actualizar parámetros con learning rate adaptativo =====\n",
    "                # θ = θ - (α / √(G + ε)) * ∇L\n",
    "                # El denominador √(G + ε) adapta el learning rate para cada parámetro\n",
    "                \n",
    "                # Learning rate adaptativo para cada peso\n",
    "                adapted_lr_w = self.learning_rate / (np.sqrt(self.G_w + self.epsilon))\n",
    "                adapted_lr_b = self.learning_rate / (np.sqrt(self.G_b + self.epsilon))\n",
    "                \n",
    "                # Actualizar usando learning rates adaptativos\n",
    "                self.weights -= adapted_lr_w * dw\n",
    "                self.bias -= adapted_lr_b * db\n",
    "            \n",
    "            # Calcular pérdida en train\n",
    "            y_train_pred = self._forward(X_train)\n",
    "            train_loss = self._compute_loss(y_train, y_train_pred)\n",
    "            self.train_losses.append(train_loss)\n",
    "            \n",
    "            # Calcular pérdida en validación (si se proporciona)\n",
    "            if X_valid is not None and y_valid is not None:\n",
    "                y_valid_pred = self._forward(X_valid)\n",
    "                valid_loss = self._compute_loss(y_valid, y_valid_pred)\n",
    "                self.valid_losses.append(valid_loss)\n",
    "                \n",
    "                if (epoch + 1) % 10 == 0:\n",
    "                    print(f\"Época {epoch+1}/{self.n_epochs} - Train Loss: {train_loss:.6f} - Valid Loss: {valid_loss:.6f}\")\n",
    "            else:\n",
    "                if (epoch + 1) % 10 == 0:\n",
    "                    print(f\"Época {epoch+1}/{self.n_epochs} - Train Loss: {train_loss:.6f}\")\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Hacer predicciones\"\"\"\n",
    "        return self._forward(X)\n",
    "    \n",
    "    def evaluate(self, X, y):\n",
    "        \"\"\"Evaluar el modelo\"\"\"\n",
    "        y_pred = self.predict(X)\n",
    "        mse = self._compute_loss(y, y_pred)\n",
    "        rmse = np.sqrt(mse)\n",
    "        \n",
    "        # R² score\n",
    "        ss_tot = np.sum((y - np.mean(y)) ** 2)\n",
    "        ss_res = np.sum((y - y_pred) ** 2)\n",
    "        r2 = 1 - (ss_res / ss_tot)\n",
    "        \n",
    "        return {'MSE': mse, 'RMSE': rmse, 'R2': r2}\n",
    "    \n",
    "    def get_effective_learning_rates(self):\n",
    "        \"\"\"Obtener los learning rates efectivos actuales para cada parámetro\"\"\"\n",
    "        lr_w = self.learning_rate / (np.sqrt(self.G_w + self.epsilon))\n",
    "        lr_b = self.learning_rate / (np.sqrt(self.G_b + self.epsilon))\n",
    "        return lr_w, lr_b\n",
    "\n",
    "print(\"Clase AdaGradRegressor definida exitosamente\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7467790b",
   "metadata": {},
   "source": [
    "## 6. Entrenar el Modelo con AdaGrad\n",
    "\n",
    "### Hiperparámetros:\n",
    "- **learning_rate**: 0.1 (más alto que SGD porque se adapta automáticamente)\n",
    "- **n_epochs**: 200 (número de épocas)\n",
    "- **batch_size**: 32 (tamaño del mini-batch)\n",
    "- **epsilon**: 1e-8 (para estabilidad numérica)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c7bba659",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ENTRENAMIENTO DEL MODELO ADAGRAD\n",
      "============================================================\n",
      "Época 10/200 - Train Loss: 0.971138 - Valid Loss: 0.861766\n",
      "Época 20/200 - Train Loss: 0.971134 - Valid Loss: 0.861813\n",
      "Época 30/200 - Train Loss: 0.971120 - Valid Loss: 0.862097\n",
      "Época 40/200 - Train Loss: 0.971111 - Valid Loss: 0.862301\n",
      "Época 50/200 - Train Loss: 0.971106 - Valid Loss: 0.862447\n",
      "Época 60/200 - Train Loss: 0.971101 - Valid Loss: 0.862558\n",
      "Época 70/200 - Train Loss: 0.971098 - Valid Loss: 0.862644\n",
      "Época 80/200 - Train Loss: 0.971096 - Valid Loss: 0.862715\n",
      "Época 90/200 - Train Loss: 0.971093 - Valid Loss: 0.862774\n",
      "Época 100/200 - Train Loss: 0.971092 - Valid Loss: 0.862824\n",
      "Época 110/200 - Train Loss: 0.971090 - Valid Loss: 0.862867\n",
      "Época 120/200 - Train Loss: 0.971089 - Valid Loss: 0.862904\n",
      "Época 130/200 - Train Loss: 0.971088 - Valid Loss: 0.862938\n",
      "Época 140/200 - Train Loss: 0.971087 - Valid Loss: 0.862968\n",
      "Época 150/200 - Train Loss: 0.971086 - Valid Loss: 0.862994\n",
      "Época 160/200 - Train Loss: 0.971085 - Valid Loss: 0.863019\n",
      "Época 170/200 - Train Loss: 0.971085 - Valid Loss: 0.863041\n",
      "Época 180/200 - Train Loss: 0.971084 - Valid Loss: 0.863061\n",
      "Época 190/200 - Train Loss: 0.971084 - Valid Loss: 0.863080\n",
      "Época 200/200 - Train Loss: 0.971083 - Valid Loss: 0.863097\n"
     ]
    }
   ],
   "source": [
    "# Crear y entrenar el modelo con AdaGrad\n",
    "print(\"=\"*60)\n",
    "print(\"ENTRENAMIENTO DEL MODELO ADAGRAD\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "model = AdaGradRegressor(learning_rate=0.1, n_epochs=200, batch_size=32, epsilon=1e-8)\n",
    "model.fit(X_train_norm, y_train_norm, X_valid_norm, y_valid_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1c7e42",
   "metadata": {},
   "source": [
    "## 7. Evaluación del Modelo\n",
    "\n",
    "### 7.1 Evaluación en Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9d9ad931",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "EVALUACIÓN EN CONJUNTO DE ENTRENAMIENTO\n",
      "============================================================\n",
      "MSE: 0.971083\n",
      "RMSE: 0.985435\n",
      "R2: 0.028917\n"
     ]
    }
   ],
   "source": [
    "# Evaluar en conjunto de entrenamiento\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EVALUACIÓN EN CONJUNTO DE ENTRENAMIENTO\")\n",
    "print(\"=\"*60)\n",
    "train_metrics = model.evaluate(X_train_norm, y_train_norm)\n",
    "for metric, value in train_metrics.items():\n",
    "    print(f\"{metric}: {value:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd908b2",
   "metadata": {},
   "source": [
    "### 7.2 Evaluación en Validación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0e48e221",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "EVALUACIÓN EN CONJUNTO DE VALIDACIÓN\n",
      "============================================================\n",
      "MSE: 0.863097\n",
      "RMSE: 0.929030\n",
      "R2: -0.065691\n"
     ]
    }
   ],
   "source": [
    "# Evaluar en conjunto de validación\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EVALUACIÓN EN CONJUNTO DE VALIDACIÓN\")\n",
    "print(\"=\"*60)\n",
    "valid_metrics = model.evaluate(X_valid_norm, y_valid_norm)\n",
    "for metric, value in valid_metrics.items():\n",
    "    print(f\"{metric}: {value:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e989adde",
   "metadata": {},
   "source": [
    "### 7.3 Evaluación en Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0fea221d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "EVALUACIÓN EN CONJUNTO DE PRUEBA (TEST)\n",
      "============================================================\n",
      "MSE: 0.747110\n",
      "RMSE: 0.864356\n",
      "R2: -0.053222\n"
     ]
    }
   ],
   "source": [
    "# Evaluar en conjunto de prueba\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EVALUACIÓN EN CONJUNTO DE PRUEBA (TEST)\")\n",
    "print(\"=\"*60)\n",
    "test_metrics = model.evaluate(X_test_norm, y_test_norm)\n",
    "for metric, value in test_metrics.items():\n",
    "    print(f\"{metric}: {value:.6f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
